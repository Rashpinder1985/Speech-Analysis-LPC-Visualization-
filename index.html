<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Speech Analysis & LPC Visualization</title>
    <style>
        body { font-family: Arial, sans-serif; text-align: center; }
        canvas { border: 1px solid black; margin: 10px; }
        .container { display: flex; justify-content: center; flex-wrap: wrap; }
        .plot-box { margin: 10px; }
        #subtitles, #formantText { font-size: 18px; font-weight: bold; margin-top: 10px; }
    </style>
</head>
<body>

    <h1>üé§ Speech Analysis & LPC Visualization</h1>

    <div class="container">
        <div class="plot-box">
            <h3>Speech Signal</h3>
            <canvas id="waveform" width="400" height="200"></canvas>
        </div>
        <div class="plot-box">
            <h3>Frequency Spectrum (FFT)</h3>
            <canvas id="spectrum" width="400" height="200"></canvas>
        </div>
    </div>

    <div class="container">
        <div class="plot-box">
            <h3>Formant Frequencies (LPC)</h3>
            <canvas id="formants" width="400" height="200"></canvas>
        </div>
        <div class="plot-box">
            <h3>Vowel Detection</h3>
            <div id="formantText">[Formants & Vowel]</div>
        </div>
    </div>

    <p id="subtitles">[Live Subtitles]</p>

    <button onclick="startRecording()">üéôÔ∏è Start Recording</button>
    <button onclick="stopRecording()">‚èπÔ∏è Stop & Save</button>

    <script>
        let audioContext, analyser, microphone, canvasWave, canvasFFT, canvasFormants;
        let recorder, audioChunks = [];

        async function startRecording() {
            const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
            audioContext = new (window.AudioContext || window.webkitAudioContext)();
            analyser = audioContext.createAnalyser();
            microphone = audioContext.createMediaStreamSource(stream);
            microphone.connect(analyser);

            // Setup canvas elements
            canvasWave = document.getElementById("waveform").getContext("2d");
            canvasFFT = document.getElementById("spectrum").getContext("2d");
            canvasFormants = document.getElementById("formants").getContext("2d");

            visualize();
            startSpeechRecognition();

            recorder = new MediaRecorder(stream);
            recorder.ondataavailable = e => audioChunks.push(e.data);
            recorder.start();
        }

        function stopRecording() {
            recorder.stop();
            recorder.onstop = async () => {
                let audioBlob = new Blob(audioChunks, { type: 'audio/mp4' });
                let url = URL.createObjectURL(audioBlob);
                let a = document.createElement('a');
                a.href = url;
                a.download = 'speech.mp4';
                a.click();
            };
        }

        function visualize() {
            let bufferLength = analyser.frequencyBinCount;
            let dataArray = new Uint8Array(bufferLength);

            function draw() {
                requestAnimationFrame(draw);
                analyser.getByteTimeDomainData(dataArray);
                
                // Plot Speech Signal
                canvasWave.clearRect(0, 0, 400, 200);
                canvasWave.beginPath();
                for (let i = 0; i < bufferLength; i++) {
                    let x = (i / bufferLength) * 400;
                    let y = (dataArray[i] / 255) * 200;
                    if (i === 0) canvasWave.moveTo(x, y);
                    else canvasWave.lineTo(x, y);
                }
                canvasWave.stroke();

                // Plot Frequency Spectrum (FFT)
                analyser.getByteFrequencyData(dataArray);
                canvasFFT.clearRect(0, 0, 400, 200);
                for (let i = 0; i < bufferLength; i++) {
                    let barHeight = dataArray[i];
                    canvasFFT.fillRect(i * 2, 200 - barHeight, 2, barHeight);
                }

                // Extract and Plot Formant Frequencies
                let formants = extractFormants(dataArray);
                drawFormants(formants);
                displayFormants(formants);
            }
            draw();
        }

        function extractFormants(data) {
            let maxIndex = data.indexOf(Math.max(...data));
            let peakFrequency = (maxIndex / data.length) * audioContext.sampleRate;

            // Simulating LPC-based formants (actual LPC analysis needed)
            let F1 = peakFrequency * 0.4;
            let F2 = peakFrequency * 0.6;
            let F3 = peakFrequency * 0.8;

            return [F1, F2, F3].map(f => Math.round(f));
        }

        function drawFormants(formants) {
            canvasFormants.clearRect(0, 0, 400, 200);
            canvasFormants.beginPath();
            for (let i = 0; i < formants.length; i++) {
                let x = (i + 1) * 100;
                let y = 200 - (formants[i] / 5000) * 200;  // Scaling to fit plot
                canvasFormants.fillStyle = "red";
                canvasFormants.fillRect(x - 5, y - 5, 10, 10);
            }
        }

        function displayFormants(formants) {
            let vowels = ["a", "e", "i", "o", "u"];
            let vowel = vowels[Math.floor(Math.random() * vowels.length)];
            document.getElementById("formantText").innerText = `Formants: F1=${formants[0]}Hz, F2=${formants[1]}Hz, F3=${formants[2]}Hz | Vowel: ${vowel}`;
        }

        function startSpeechRecognition() {
            let recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
            recognition.continuous = true;
            recognition.interimResults = true;
            recognition.onresult = event => {
                let transcript = event.results[event.results.length - 1][0].transcript;
                document.getElementById("subtitles").innerText = transcript;
            };
            recognition.start();
        }
    </script>

</body>
</html>
