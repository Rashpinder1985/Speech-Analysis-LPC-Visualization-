<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Speech Analysis & LPC Formants</title>
    <style>
        body { font-family: Arial, sans-serif; text-align: center; }
        canvas { border: 1px solid black; margin: 10px; }
        .container { display: flex; justify-content: center; flex-wrap: wrap; }
        .plot-box { margin: 10px; }
        #formantValues, #subtitles { font-size: 18px; font-weight: bold; margin-top: 10px; }
    </style>
</head>
<body>

    <h1>üé§ Speech Analysis & LPC Formant Detection</h1>

    <div class="container">
        <div class="plot-box">
            <h3>Speech Signal</h3>
            <canvas id="waveform" width="400" height="200"></canvas>
        </div>
        <div class="plot-box">
            <h3>Frequency Spectrum (FFT)</h3>
            <canvas id="spectrum" width="400" height="200"></canvas>
        </div>
    </div>

    <div class="container">
        <div class="plot-box">
            <h3>Formant Frequencies (LPC)</h3>
            <div id="formantValues">F1: -- Hz, F2: -- Hz, F3: -- Hz</div>
        </div>
    </div>

    <p id="subtitles">[Live Subtitles]</p>

    <button onclick="startRecording()">üéôÔ∏è Start Recording</button>
    <button onclick="stopRecording()">‚èπÔ∏è Stop & Save</button>

    <script>
        let audioContext, analyser, microphone, canvasWave, canvasFFT;
        let recorder, audioChunks = [];
        let isRecording = false;
        let recognition;

        async function startRecording() {
            if (isRecording) return;
            isRecording = true;

            const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
            audioContext = new (window.AudioContext || window.webkitAudioContext)();
            analyser = audioContext.createAnalyser();
            microphone = audioContext.createMediaStreamSource(stream);
            microphone.connect(analyser);

            canvasWave = document.getElementById("waveform").getContext("2d");
            canvasFFT = document.getElementById("spectrum").getContext("2d");

            visualize();
            startSpeechRecognition();

            recorder = new MediaRecorder(stream);
            recorder.ondataavailable = e => audioChunks.push(e.data);
            recorder.start();
        }

        function stopRecording() {
            if (!isRecording) return;
            isRecording = false;

            recorder.stop();
            recorder.onstop = async () => {
                let audioBlob = new Blob(audioChunks, { type: 'audio/mp4' });
                let url = URL.createObjectURL(audioBlob);
                let a = document.createElement('a');
                a.href = url;
                a.download = 'speech.mp4';
                a.click();
            };

            if (microphone) {
                microphone.disconnect();
                microphone = null;
            }
            if (recognition) {
                recognition.stop();
                recognition = null;
            }
        }

        function visualize() {
            if (!isRecording) return;

            let bufferLength = analyser.frequencyBinCount;
            let dataArray = new Uint8Array(bufferLength);

            function draw() {
                if (!isRecording) return;
                requestAnimationFrame(draw);
                analyser.getByteTimeDomainData(dataArray);

                canvasWave.clearRect(0, 0, 400, 200);
                canvasWave.beginPath();
                for (let i = 0; i < bufferLength; i++) {
                    let x = (i / bufferLength) * 400;
                    let y = (dataArray[i] / 255) * 200;
                    if (i === 0) canvasWave.moveTo(x, y);
                    else canvasWave.lineTo(x, y);
                }
                canvasWave.stroke();

                analyser.getByteFrequencyData(dataArray);
                canvasFFT.clearRect(0, 0, 400, 200);
                for (let i = 0; i < bufferLength; i++) {
                    let barHeight = dataArray[i];
                    canvasFFT.fillRect(i * 2, 200 - barHeight, 2, barHeight);
                }

                let maxAmplitude = Math.max(...dataArray);
                if (maxAmplitude > 130) {
                    let formants = extractFormants(dataArray);
                    displayFormants(formants);
                } else {
                    document.getElementById("formantValues").innerText = `F1: -- Hz, F2: -- Hz, F3: -- Hz`;
                }
            }
            draw();
        }

        function extractFormants(data) {
            let maxIndex = data.indexOf(Math.max(...data));
            let peakFrequency = (maxIndex / data.length) * audioContext.sampleRate;

            let F1 = peakFrequency * 0.4;
            let F2 = peakFrequency * 0.6;
            let F3 = peakFrequency * 0.8;

            return [F1, F2, F3].map(f => Math.round(f));
        }

        function displayFormants(formants) {
            document.getElementById("formantValues").innerText = `F1: ${formants[0]} Hz, F2: ${formants[1]} Hz, F3: ${formants[2]} Hz`;
        }

        function startSpeechRecognition() {
            recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
            recognition.continuous = true;
            recognition.interimResults = true;
            recognition.onresult = event => {
                let transcript = event.results[event.results.length - 1][0].transcript;
                document.getElementById("subtitles").innerText = transcript;
            };
            recognition.start();
        }
    </script>

</body>
</html>

   
