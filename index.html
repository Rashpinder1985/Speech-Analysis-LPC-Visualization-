<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Speech Analysis & LPC Formants</title>
    <style>
        body { font-family: Arial, sans-serif; text-align: center; }
        canvas { border: 1px solid black; margin: 10px; }
        .container { display: flex; justify-content: center; flex-wrap: wrap; }
        .plot-box { margin: 10px; }
        #formantValues, #vowelDetected, #subtitles { font-size: 18px; font-weight: bold; margin-top: 10px; }
    </style>
</head>
<body>

    <h1>üé§ Speech Analysis & LPC Formant Detection</h1>

    <div class="container">
        <div class="plot-box">
            <h3>Speech Signal</h3>
            <canvas id="waveform" width="400" height="200"></canvas>
        </div>
        <div class="plot-box">
            <h3>Frequency Spectrum (FFT)</h3>
            <canvas id="spectrum" width="400" height="200"></canvas>
        </div>
    </div>

    <div class="container">
        <div class="plot-box">
            <h3>Formant Frequencies (LPC)</h3>
            <div id="formantValues">F1: -- Hz, F2: -- Hz, F3: -- Hz</div>
        </div>
    </div>

    <div class="container">
        <div class="plot-box">
            <h3>Detected Vowel</h3>
            <div id="vowelDetected">--</div>
        </div>
    </div>

    <p id="subtitles">[Live Subtitles]</p>

    <button onclick="startRecording()">üéôÔ∏è Start Recording</button>
    <button onclick="stopRecording()">‚èπÔ∏è Stop & Save</button>

    <script>
        let audioContext, analyser, microphone, canvasWave, canvasFFT;
        let recorder, audioChunks = [];

        async function startRecording() {
            const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
            audioContext = new (window.AudioContext || window.webkitAudioContext)();
            analyser = audioContext.createAnalyser();
            microphone = audioContext.createMediaStreamSource(stream);
            microphone.connect(analyser);

            canvasWave = document.getElementById("waveform").getContext("2d");
            canvasFFT = document.getElementById("spectrum").getContext("2d");

            visualize();
            startSpeechRecognition();

            recorder = new MediaRecorder(stream);
            recorder.ondataavailable = e => audioChunks.push(e.data);
            recorder.start();
        }

        function stopRecording() {
            recorder.stop();
            recorder.onstop = async () => {
                let audioBlob = new Blob(audioChunks, { type: 'audio/mp4' });
                let url = URL.createObjectURL(audioBlob);
                let a = document.createElement('a');
                a.href = url;
                a.download = 'speech.mp4';
                a.click();
            };
        }

        function visualize() {
            let bufferLength = analyser.frequencyBinCount;
            let dataArray = new Uint8Array(bufferLength);

            function draw() {
                requestAnimationFrame(draw);
                analyser.getByteTimeDomainData(dataArray);
                
                // Plot Speech Signal
                canvasWave.clearRect(0, 0, 400, 200);
                canvasWave.beginPath();
                for (let i = 0; i < bufferLength; i++) {
                    let x = (i / bufferLength) * 400;
                    let y = (dataArray[i] / 255) * 200;
                    if (i === 0) canvasWave.moveTo(x, y);
                    else canvasWave.lineTo(x, y);
                }
                canvasWave.stroke();

                // Plot Frequency Spectrum (FFT)
                analyser.getByteFrequencyData(dataArray);
                canvasFFT.clearRect(0, 0, 400, 200);
                for (let i = 0; i < bufferLength; i++) {
                    let barHeight = dataArray[i];
                    canvasFFT.fillRect(i * 2, 200 - barHeight, 2, barHeight);
                }

                // Extract and Display Formants
                let formants = extractFormants(dataArray);
                displayFormants(formants);
                detectVowel(formants);
            }
            draw();
        }

        function extractFormants(data) {
            let maxIndex = data.indexOf(Math.max(...data));
            let peakFrequency = (maxIndex / data.length) * audioContext.sampleRate;

            // Simulated LPC formant extraction
            let F1 = peakFrequency * 0.4;
            let F2 = peakFrequency * 0.6;
            let F3 = peakFrequency * 0.8;

            return [F1, F2, F3].map(f => Math.round(f));
        }

        function displayFormants(formants) {
            document.getElementById("formantValues").innerText = `F1: ${formants[0]} Hz, F2: ${formants[1]} Hz, F3: ${formants[2]} Hz`;
        }

        function detectVowel(formants) {
            let F1 = formants[0];
            let F2 = formants[1];
            let vowel = "--";

            // Basic Vowel Classification based on F1 and F2 (approximate values)
            if (F1 > 500 && F1 < 800 && F2 > 1000 && F2 < 1300) vowel = "A";
            else if (F1 > 300 && F1 < 500 && F2 > 2000 && F2 < 2500) vowel = "I";
            else if (F1 > 300 && F1 < 500 && F2 > 700 && F2 < 1200) vowel = "U";
            else if (F1 > 500 && F1 < 700 && F2 > 1600 && F2 < 2300) vowel = "E";
            else if (F1 > 400 && F1 < 600 && F2 > 900 && F2 < 1500) vowel = "O";

            document.getElementById("vowelDetected").innerText = `Detected Vowel: ${vowel}`;
        }

        function startSpeechRecognition() {
            let recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
            recognition.continuous = true;
            recognition.interimResults = true;
            recognition.onresult = event => {
                let transcript = event.results[event.results.length - 1][0].transcript;
                document.getElementById("subtitles").innerText = transcript;
            };
            recognition.start();
        }
    </script>

</body>
</html>

   
